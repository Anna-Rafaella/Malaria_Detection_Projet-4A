{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"f4c1f6af83224d6a915fa785d15cd5f7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c72c3385683441659f7bd77087318c5b","IPY_MODEL_d527b769b08044baa340d56af72160d5","IPY_MODEL_6a725f164e104099a302f32f1dee322f"],"layout":"IPY_MODEL_ab98bf152f364834a7c16e9554915b65"}},"c72c3385683441659f7bd77087318c5b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5abc0ce23cbf4df4a7f034b1eaf38dea","placeholder":"​","style":"IPY_MODEL_63b90fc4e91c41e2b6fa2be242a99e83","value":"100%"}},"d527b769b08044baa340d56af72160d5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee2dd63ee58440bf8576de295c2f526c","max":1544,"min":0,"orientation":"horizontal","style":"IPY_MODEL_53907b5594b84194b007c2d5799bd870","value":1544}},"6a725f164e104099a302f32f1dee322f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8f87c4b6f444d6b9aacded4f2d6433c","placeholder":"​","style":"IPY_MODEL_c188fd265944442bb96767033c9dcc3d","value":" 1544/1544 [00:02&lt;00:00, 415.23it/s]"}},"ab98bf152f364834a7c16e9554915b65":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5abc0ce23cbf4df4a7f034b1eaf38dea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63b90fc4e91c41e2b6fa2be242a99e83":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ee2dd63ee58440bf8576de295c2f526c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"53907b5594b84194b007c2d5799bd870":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e8f87c4b6f444d6b9aacded4f2d6433c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c188fd265944442bb96767033c9dcc3d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eb9931ec89104ac3b2860dee51c70053":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0bce3ce11c1640169764d836d04ed31c","IPY_MODEL_b554f98d60d84941a3354b65732ef011","IPY_MODEL_c487b4e385c447698783b9d0afe09493"],"layout":"IPY_MODEL_e64474a9cf00467cb0172c7da48ae276"}},"0bce3ce11c1640169764d836d04ed31c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a75c9ce8f0244e29af4d0287ae086081","placeholder":"​","style":"IPY_MODEL_1e2b1994a9c44057a87d5cf30f92e7f4","value":"100%"}},"b554f98d60d84941a3354b65732ef011":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_09c620ec1a82421a9133ca0aff7e4288","max":515,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c3e9afebb36d47849284643363007e30","value":515}},"c487b4e385c447698783b9d0afe09493":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3562ee9f3c294f249b43f4110188eae0","placeholder":"​","style":"IPY_MODEL_0a6dad4012d646a2b44a7ee87214955b","value":" 515/515 [00:00&lt;00:00, 651.03it/s]"}},"e64474a9cf00467cb0172c7da48ae276":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a75c9ce8f0244e29af4d0287ae086081":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e2b1994a9c44057a87d5cf30f92e7f4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"09c620ec1a82421a9133ca0aff7e4288":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3e9afebb36d47849284643363007e30":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3562ee9f3c294f249b43f4110188eae0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a6dad4012d646a2b44a7ee87214955b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"30d46873186046daa6ec24a98978ea5a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5fd578e8d5d6423b9cd107468b243f1b","IPY_MODEL_453cc32776b04e5cb502d44dfe01b27f","IPY_MODEL_a6356254fb2645b6a54fc17fbf72e8c7"],"layout":"IPY_MODEL_f05d3273f3154e7bb38926b6da453467"}},"5fd578e8d5d6423b9cd107468b243f1b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ca5db94c18b4b758b2dafa74445bb41","placeholder":"​","style":"IPY_MODEL_29e661853f854c15ab97985f55a82890","value":"100%"}},"453cc32776b04e5cb502d44dfe01b27f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_081693f6557a4dd19cecfa00f0b491cb","max":1178,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b98150afc60e4798ae5f38ae611d8e44","value":1178}},"a6356254fb2645b6a54fc17fbf72e8c7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ced0e4a56a2f4980a55bd1b2240a2b11","placeholder":"​","style":"IPY_MODEL_0616ff2d27a44b27b611416db644d5e3","value":" 1178/1178 [00:03&lt;00:00, 241.60it/s]"}},"f05d3273f3154e7bb38926b6da453467":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ca5db94c18b4b758b2dafa74445bb41":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29e661853f854c15ab97985f55a82890":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"081693f6557a4dd19cecfa00f0b491cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b98150afc60e4798ae5f38ae611d8e44":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ced0e4a56a2f4980a55bd1b2240a2b11":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0616ff2d27a44b27b611416db644d5e3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"10ce760a05c04ddf9fff851ddeef7719":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fe8d6d29dd0f43ab97ea3a9d6a03b04d","IPY_MODEL_59c2675722ae457b8b9422d98f3cb335","IPY_MODEL_61be39617d294563be59109d967813be"],"layout":"IPY_MODEL_f262235aec01445792c6baf6745591d0"}},"fe8d6d29dd0f43ab97ea3a9d6a03b04d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac1ba5ea211e4149b6e7399fe8e8e497","placeholder":"​","style":"IPY_MODEL_fceeebb9b8764fd8abb7396ee91651de","value":"100%"}},"59c2675722ae457b8b9422d98f3cb335":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8681fa9b0154365af2db81b4867a66c","max":17354,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a775675e69cb48b5b99b6a3b9854898c","value":17354}},"61be39617d294563be59109d967813be":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7cd6e4cb7f943dbac084ebe28ffda5c","placeholder":"​","style":"IPY_MODEL_a37e19f55c9a4db4a93c056054c0ab60","value":" 17354/17354 [01:41&lt;00:00, 177.21it/s]"}},"f262235aec01445792c6baf6745591d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac1ba5ea211e4149b6e7399fe8e8e497":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fceeebb9b8764fd8abb7396ee91651de":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f8681fa9b0154365af2db81b4867a66c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a775675e69cb48b5b99b6a3b9854898c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d7cd6e4cb7f943dbac084ebe28ffda5c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a37e19f55c9a4db4a93c056054c0ab60":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aece70afa6124ae48a96d047274a7383":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6afc4124cf7446dead944f2fb7e0bd78","IPY_MODEL_3920baa9edf040589219b3ac064b5f30","IPY_MODEL_5174be49f05747e696ce6ec9d8f0bccc"],"layout":"IPY_MODEL_19b9d7a0f6454e98972f2e2dde3fa1cc"}},"6afc4124cf7446dead944f2fb7e0bd78":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1636ca90c8cc4111978301887cbb70f2","placeholder":"​","style":"IPY_MODEL_3d7be0c79f9c4cd1a72f287a56a7f386","value":"100%"}},"3920baa9edf040589219b3ac064b5f30":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b92bc644355465092684fcf085f8bd7","max":5488,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d81a373dbcba46cf83323492b3874f08","value":5488}},"5174be49f05747e696ce6ec9d8f0bccc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_254cf6c8f775462f8c24eb9b9af84134","placeholder":"​","style":"IPY_MODEL_dbfbceb66b97451aa5baeb090294b0cf","value":" 5488/5488 [00:32&lt;00:00, 167.39it/s]"}},"19b9d7a0f6454e98972f2e2dde3fa1cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1636ca90c8cc4111978301887cbb70f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d7be0c79f9c4cd1a72f287a56a7f386":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2b92bc644355465092684fcf085f8bd7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d81a373dbcba46cf83323492b3874f08":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"254cf6c8f775462f8c24eb9b9af84134":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dbfbceb66b97451aa5baeb090294b0cf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9891910,"sourceType":"datasetVersion","datasetId":5755933},{"sourceId":165603,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":140908,"modelId":163511}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Check GPU type\nprint(\"ça marche !\")\n!pip install nvidia-smi\n\n# nvidia-smi est un outil en ligne de commande fourni par NVIDIA pour interagir avec les GPU NVIDIA installés sur un système.\n# pas nécessaire pour kaggle car il gère l'accès aux ressources GPU","metadata":{"_uuid":"c4a70ab2-fc0e-43ca-ac18-b34f19611da4","_cell_guid":"0b66d84a-0361-45db-bacc-df57be92efde","collapsed":false,"id":"C2JGBuAeUDi7","outputId":"e4aef05a-52cf-43e9-accd-696e4c82ccd5","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-14T19:46:05.438635Z","iopub.execute_input":"2024-11-14T19:46:05.439271Z","iopub.status.idle":"2024-11-14T19:46:18.803031Z","shell.execute_reply.started":"2024-11-14T19:46:05.439228Z","shell.execute_reply":"2024-11-14T19:46:18.801925Z"},"trusted":true},"outputs":[{"name":"stdout","text":"ça marche !\nCollecting nvidia-smi\n  Downloading nvidia_smi-0.1.3-py36-none-any.whl.metadata (2.1 kB)\nRequirement already satisfied: numpy>=1.16.2 in /opt/conda/lib/python3.10/site-packages (from nvidia-smi) (1.26.4)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from nvidia-smi) (1.16.0)\nCollecting sorcery>=0.1.0 (from nvidia-smi)\n  Downloading sorcery-0.2.2-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: pytest>=4.3.1 in /opt/conda/lib/python3.10/site-packages (from nvidia-smi) (8.3.3)\nRequirement already satisfied: iniconfig in /opt/conda/lib/python3.10/site-packages (from pytest>=4.3.1->nvidia-smi) (2.0.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from pytest>=4.3.1->nvidia-smi) (21.3)\nRequirement already satisfied: pluggy<2,>=1.5 in /opt/conda/lib/python3.10/site-packages (from pytest>=4.3.1->nvidia-smi) (1.5.0)\nRequirement already satisfied: exceptiongroup>=1.0.0rc8 in /opt/conda/lib/python3.10/site-packages (from pytest>=4.3.1->nvidia-smi) (1.2.0)\nRequirement already satisfied: tomli>=1 in /opt/conda/lib/python3.10/site-packages (from pytest>=4.3.1->nvidia-smi) (2.0.1)\nRequirement already satisfied: executing in /opt/conda/lib/python3.10/site-packages (from sorcery>=0.1.0->nvidia-smi) (2.0.1)\nCollecting littleutils>=0.2.1 (from sorcery>=0.1.0->nvidia-smi)\n  Downloading littleutils-0.2.4-py3-none-any.whl.metadata (679 bytes)\nRequirement already satisfied: asttokens in /opt/conda/lib/python3.10/site-packages (from sorcery>=0.1.0->nvidia-smi) (2.4.1)\nRequirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from sorcery>=0.1.0->nvidia-smi) (1.16.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->pytest>=4.3.1->nvidia-smi) (3.1.2)\nDownloading nvidia_smi-0.1.3-py36-none-any.whl (11 kB)\nDownloading sorcery-0.2.2-py3-none-any.whl (16 kB)\nDownloading littleutils-0.2.4-py3-none-any.whl (8.1 kB)\nInstalling collected packages: littleutils, sorcery, nvidia-smi\nSuccessfully installed littleutils-0.2.4 nvidia-smi-0.1.3 sorcery-0.2.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install --upgrade pip\n\n# Install ultralytics\n!pip -q install  ultralytics\n\n!pip install ipywidgets\n!pip install jupyter\n\n# !pip install --upgrade jupyter\n# !pip install --upgrade ipywidgets\n\n#Bibliothèque pour la modification des images\n!pip install -U albumentations\n\n#Bibliothèque pour le réequilibrage des classes des données\n!pip install imbalanced-learn\n\n!ultralytics.check()\n\n!pip install ipywidgets\n!pip install jupyter\n!pip install pillow \n\n#ultralytics est une bibliothèque Python qui fournit des modèles de détection d'objets, de segmentation sémantique et d'autres tâches d'apprentissage profond.","metadata":{"_uuid":"0dd4c84a-43f1-4d20-bbb6-04c93d600dab","_cell_guid":"a9644160-881e-4671-95d9-79b003ba98e5","collapsed":false,"id":"AK-6-eQqhTeV","outputId":"8260324d-4ca5-4cd9-c5e1-efbf047d16d7","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-14T19:46:18.805312Z","iopub.execute_input":"2024-11-14T19:46:18.805632Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (24.0)\nCollecting pip\n  Downloading pip-24.3.1-py3-none-any.whl.metadata (3.7 kB)\nDownloading pip-24.3.1-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 24.0\n    Uninstalling pip-24.0:\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Import libraries\nimport pandas as pd                                                # Manipulation de données sous forme de tableaux\nimport os                                                          # Interactions avec le système d'exploitation\nfrom pathlib import Path                                           # Manipulation de chemins de fichiers\nimport shutil                                                      # Opérations de copie et déplacement de fichiers\nfrom sklearn.model_selection import train_test_split               # Division d'un jeu de données en ensembles d'entraînement et de test\nfrom tqdm.notebook import tqdm                                     # Affichage de barres de progression dans un notebook Jupyter\nimport cv2                                                         # Traitement d'images\nimport yaml                                                        # Lecture et écriture de fichiers de configuration au format YAML\nimport matplotlib.pyplot as plt                                    # Création de visualisations\nfrom ultralytics import YOLO                                       # Utilisation de modèles de détection d'objets\nimport multiprocessing                                             # Calculs en parallèle\nfrom kaggle_secrets import UserSecretsClient\nimport torch\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport numpy as np\nfrom collections import Counter\nimport numpy as np\nfrom skimage.exposure import match_histograms\nfrom tqdm import tqdm\nimport concurrent.futures\nfrom PIL import Image","metadata":{"_uuid":"4a596abf-9951-4432-85c2-263191caaba7","_cell_guid":"0a9d5e56-b336-4243-b07e-c7d4ca8ca1b4","collapsed":false,"id":"YEIlI7wOGP-R","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Lister tout le contenu du répertoire\n!ls /kaggle/input/malaria-data\n\n# Path to where your data is stored\nDATA_DIR = Path('/kaggle/input/malaria-data') # Définit le chemin vers le répertoire contenant les données\n\n# Preview data files available\nos.listdir(DATA_DIR) # Affiche la liste des fichiers disponibles dans le répertoire des données","metadata":{"_uuid":"4a62489a-c65a-41eb-ab5b-c6bb6fa35c03","_cell_guid":"36444792-0831-4b70-b3e4-8ae1554e47bd","collapsed":false,"id":"Ozqm5yKJGuV4","outputId":"c03df89f-24e4-405a-f241-fe6d7d05cf3d","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set up directoris for training a yolo model\n\n# Images directories\nDATASET_DIR = Path('datasets/dataset')\nIMAGES_DIR = DATASET_DIR / 'images'\nTRAIN_IMAGES_DIR = IMAGES_DIR / 'train'\nVAL_IMAGES_DIR = IMAGES_DIR / 'val'\nTEST_IMAGES_DIR = IMAGES_DIR / 'test'\n\nVAL_IMAGES_0_DIR = VAL_IMAGES_DIR / 'class_0'\nVAL_IMAGES_1_DIR = VAL_IMAGES_DIR / 'class_1'\nTRAIN_IMAGES_0_DIR = TRAIN_IMAGES_DIR / 'class_0'\nTRAIN_IMAGES_1_DIR = TRAIN_IMAGES_DIR / 'class_1'\n\n# Labels directories\nLABELS_DIR = DATASET_DIR / 'labels'\nTRAIN_LABELS_DIR = LABELS_DIR / 'train'\nVAL_LABELS_DIR = LABELS_DIR / 'val'\nTEST_LABELS_DIR = LABELS_DIR / 'test'\n\nTRAIN_LABELS_0_DIR = TRAIN_LABELS_DIR / 'class_0'\nTRAIN_LABELS_1_DIR = TRAIN_LABELS_DIR / 'class_1'\nVAL_LABELS_0_DIR = VAL_LABELS_DIR / 'class_0'\nVAL_LABELS_1_DIR = VAL_LABELS_DIR / 'class_1'\n","metadata":{"_uuid":"fdf50b6f-c287-42d7-bdd1-874abd9dc7a5","_cell_guid":"fcfbf459-4c47-4400-91ca-3379d37a01ea","collapsed":false,"id":"oYL1hmTCP6d0","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Unzip images to 'images' dir\nif (DATA_DIR/ 'images.zip').exists():\n    shutil.unpack_archive(DATASET_DIR / 'images.zip', 'IMAGES_DIR')  # Dézippe le fichier 'images.zip' vers le répertoire 'IMAGES_DIR'\nelse:\n    print(\"fichier déjà dézippé\")             # Si le fichier est déjà dézippé, affiche un message informant que le fichier est déjà extrait","metadata":{"_uuid":"a4b52f96-19d4-4e51-9ed9-f7ddd1b53abd","_cell_guid":"85688fa8-c6fd-45bb-8e4d-f7cd88d3d26d","collapsed":false,"id":"COCyht5BGuQV","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load train and test files\ntrain = pd.read_csv(DATA_DIR / 'Train.csv')          # Charge le fichier CSV contenant les données d'entraînement\ntest = pd.read_csv(DATA_DIR / 'Test.csv')            # Charge le fichier CSV contenant les données de test\nss = pd.read_csv(DATA_DIR / 'SampleSubmission.csv')  # Charge le fichier CSV contenant un exemple de soumission\n\n# Add an image_path column\ntrain['image_path'] = [Path('../input/malaria-data/images/' + x) for x in train.Image_ID]   # Ajoute une colonne 'image_path' avec le chemin des images d'entraînement\ntest['image_path'] = [Path('../input/malaria-data/images/' + x) for x in test.Image_ID]    # Ajoute une colonne 'image_path' avec le chemin des images de test\n\n# Map str classes to ints (label encoding targets)\ntrain['class_id'] = train['class'].map({'Trophozoite': 0, 'WBC': 1, 'NEG': 2}) # Encode les classes sous forme d'entiers (0, 1, 2)\n\n# Preview the head of the train set\ntrain.head()            # Affiche les premières lignes du jeu de données d'entraînement","metadata":{"_uuid":"f00e67ab-79f7-4f4e-8cb3-c0fd1332c50c","_cell_guid":"c9952460-4a2a-4f23-a915-3ea387a39329","collapsed":false,"id":"VIBthAgmRcYW","outputId":"4e93bd01-df2d-46c9-f818-73ae70fc1f4e","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Filter out NEG class (Negative images - this images have no Trophozoite nor WBC in them)\n# Yolo reccomends that when creating labels for negative images, you leave them empty\n# https://github.com/ultralytics/yolov5/issues/3218\n# As this is a starter nb, we will ignore the negative images\ntrain = train[~(train['class'] == 'NEG')].reset_index(drop = True)       # Filtre le jeu de données d'entraînement pour ne garder que les images \n                                                                         # qui ne sont pas de classe 'NEG' (négatif), \n                                                                        # puis réinitialise l'index des lignes","metadata":{"_uuid":"63a135e9-9363-4a64-8bbe-c94362eb94fc","_cell_guid":"20956661-44b0-437e-abd3-c747de57f002","collapsed":false,"id":"Cro7Tqtzxd_g","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split data into training and validation\ntrain_unique_imgs_df = train.drop_duplicates(subset = ['Image_ID'], ignore_index = True) # Crée un DataFrame contenant les images d'entraînement uniques\nX_train, X_val = train_test_split(train_unique_imgs_df, test_size = 0.25, stratify=train_unique_imgs_df['class'], random_state=42) # Divise les images d'entraînement uniques en ensembles d'entraînement et de validation, \n                                                                                                                                   # en respectant la stratification des classes et en utilisant un seed aléatoire\n\n\n# Cette syntaxe est utilisée pour le filtrage en pandas. \n# Elle sélectionne toutes les lignes du DataFrame train où le masque booléen est True.    \nX_train = train[train.Image_ID.isin(X_train.Image_ID)]\nX_val = train[train.Image_ID.isin(X_val.Image_ID)]\n\n# Check shapes of training and validation data\nX_train.shape, X_val.shape ","metadata":{"_uuid":"719a8699-369f-4ddd-ac25-8cde8af235d4","_cell_guid":"3ae53b0a-937f-4a97-9e2d-3bef882ecb25","collapsed":false,"id":"17zUCoWWT6JO","outputId":"57d1da35-b3e8-412a-9869-dc084463cba1","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train.head()","metadata":{"_uuid":"4f9235dc-04c5-4940-8a5b-3559127b9f1c","_cell_guid":"feeabab3-ade1-4504-a175-a3952ddb81d0","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# def match_histogram(image, reference):\n#     # Convertir les images BGR en RGB (OpenCV utilise BGR par défaut)\n#     image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n#     reference_rgb = cv2.cvtColor(reference, cv2.COLOR_BGR2RGB)\n    \n#     # Appliquer la correspondance d'histogramme\n#     matched = match_histograms(image_rgb, reference_rgb, multichannel=True)\n    \n#     # Reconvertir l'image correspondante en BGR pour OpenCV\n#     matched_bgr = cv2.cvtColor((matched * 255).astype(np.uint8), cv2.COLOR_RGB2BGR)\n#     return matched_bgr\n\n# def batch_match_histograms(input_folder, output_folder, reference_image_path):\n#     os.makedirs(output_folder, exist_ok=True)\n    \n#     # Charger l'image de référence\n#     reference_image = cv2.imread(reference_image_path)\n    \n#     # Obtenir la liste des fichiers d'images\n#     image_files = [f for f in os.listdir(input_folder) if f.endswith('.jpg') or f.endswith('.png')]\n    \n#     for image_file in tqdm(image_files, desc=\"Normalisation des images par correspondance d'histogramme\"):\n#         input_path = os.path.join(input_folder, image_file)\n#         output_path = os.path.join(output_folder, image_file)\n        \n#         # Charger l'image actuelle\n#         image = cv2.imread(input_path)\n        \n#         # Appliquer la correspondance d'histogramme\n#         normalized_image = match_histogram(image, reference_image)\n        \n#         # Sauvegarder l'image normalisée\n#         cv2.imwrite(output_path, normalized_image)\n\n# # Utilisation\n# input_folder = '../input/images/'  # Chemin vers les images d'origine\n# output_folder = '../input/images/'  # Chemin vers les images normalisées\n# reference_image_path = '../input/images/id_0051lznyrt.jpg'  # Chemin vers l'image de référence\n\n# batch_match_histograms(input_folder, output_folder, reference_image_path)\nX_val.head()","metadata":{"_uuid":"81acc9fe-77d8-4b90-a6ad-981bc164585c","_cell_guid":"9e178cfd-fcf7-40a6-88d0-2d47ccb916ad","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Preview target distribution, seems there a class imbalance that needs to be handled\n# Affiche la répartition en pourcentage des classes dans les jeux de données d'entraînement et de validation\nX_train['class'].value_counts(normalize = True), X_val['class'].value_counts(normalize = True)","metadata":{"_uuid":"84942e2d-8b46-42b1-8816-314065e7fbbb","_cell_guid":"033a56d6-64cd-4b2d-9265-216ea393aef7","collapsed":false,"id":"3Vy1u_542aI7","outputId":"49a677fd-2f7c-4c00-da4e-e11bbdb8149b","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train.head()","metadata":{"_uuid":"197a663a-0f6b-4b94-bf27-314922297127","_cell_guid":"fdf08524-110f-4f08-ad2a-6895c9e63ee9","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check if dirs exist, if they do, remove them, otherwise create them.\n# This only needs to run once\nfor DIR in [TRAIN_IMAGES_DIR,VAL_IMAGES_DIR, TEST_IMAGES_DIR, TRAIN_LABELS_DIR,VAL_LABELS_DIR,TEST_LABELS_DIR,TRAIN_IMAGES_0_DIR,TRAIN_IMAGES_1_DIR,TRAIN_LABELS_0_DIR,TRAIN_LABELS_1_DIR,VAL_IMAGES_0_DIR,VAL_IMAGES_1_DIR,VAL_LABELS_0_DIR,VAL_LABELS_1_DIR]:\n  if DIR.exists():\n    shutil.rmtree(DIR)   # Si le répertoire existe, le supprime de manière récursive\n  DIR.mkdir(parents=True, exist_ok = True)   # Crée le répertoire s'il n'existe pas, avec création des répertoires parents si nécessaire","metadata":{"_uuid":"ddd398fe-03aa-4fd9-8cac-bddcdf465fae","_cell_guid":"17c7cd16-d2a4-42a3-b12b-cfd6270d45b5","collapsed":false,"id":"ePBUqMMzGuOC","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nfrom pathlib import Path\nfrom tqdm import tqdm\n\n# Définir les répertoires de destination pour chaque classe dans le dossier d'entraînement\nCLASS_0_DIR_TRAIN = TRAIN_IMAGES_0_DIR\nCLASS_1_DIR_TRAIN = TRAIN_IMAGES_1_DIR\n\n# Définir les répertoires de destination pour chaque classe dans le dossier de validation\nCLASS_0_DIR_VAL = VAL_IMAGES_0_DIR\nCLASS_1_DIR_VAL = VAL_IMAGES_1_DIR\n\n# Définir le répertoire de destination pour les images de test\nTEST_IMAGES_DIR = Path(TEST_IMAGES_DIR)\n\n# Créer tous les répertoires si ce n'est pas déjà fait\nCLASS_0_DIR_TRAIN.mkdir(parents=True, exist_ok=True)\nCLASS_1_DIR_TRAIN.mkdir(parents=True, exist_ok=True)\nCLASS_0_DIR_VAL.mkdir(parents=True, exist_ok=True)\nCLASS_1_DIR_VAL.mkdir(parents=True, exist_ok=True)\nTEST_IMAGES_DIR.mkdir(parents=True, exist_ok=True)\n\n# Copier les images d'entraînement dans les répertoires de classes correspondants\nfor _, row in tqdm(X_train.iterrows(), total=X_train.shape[0]):\n    img_path = Path(row['image_path'])  # Chemin de l'image\n    class_id = row['class_id']  # Classe de l'image\n\n    # Copier dans le dossier de classe approprié\n    if class_id == 0:\n        shutil.copy(img_path, CLASS_0_DIR_TRAIN / img_path.name)\n    elif class_id == 1:\n        shutil.copy(img_path, CLASS_1_DIR_TRAIN / img_path.name)\n\n# Copier les images de validation dans les répertoires de classes correspondants\nfor _, row in tqdm(X_val.iterrows(), total=X_val.shape[0]):\n    img_path = Path(row['image_path'])\n    class_id = row['class_id']\n\n    # Copier dans le dossier de classe approprié\n    if class_id == 0:\n        shutil.copy(img_path, CLASS_0_DIR_VAL / img_path.name)\n    elif class_id == 1:\n        shutil.copy(img_path, CLASS_1_DIR_VAL / img_path.name)\n\n# Copier les images de test dans le répertoire de test\nfor img in tqdm(test.image_path.unique()):\n    img_path = Path(img)\n    shutil.copy(img_path, TEST_IMAGES_DIR / img_path.name)\n","metadata":{"_uuid":"70851e29-04ad-477e-86f0-3c1f9d0a0922","_cell_guid":"057387fd-d12c-4e31-94cb-400fe30b0afb","collapsed":false,"id":"N4Pi3KUQIyeJ","outputId":"46875856-5b62-4d51-f4a0-be3e0c552d22","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# def convert_to_black_on_white(image):\n#     # Convertir l'image en niveaux de gris\n#     gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    \n#     # Inverser les couleurs pour avoir un fond blanc et des objets noirs\n#     inverted_image = cv2.bitwise_not(gray_image)\n    \n#     # Convertir en BGR pour rester compatible avec d'autres opérations éventuelles\n#     bgr_image = cv2.cvtColor(inverted_image, cv2.COLOR_GRAY2BGR)\n    \n#     return bgr_image\n\n# def batch_process_images(input_folder, output_folder):\n#     os.makedirs(output_folder, exist_ok=True)\n    \n#     # Obtenir la liste des fichiers d'images\n#     image_files = [f for f in os.listdir(input_folder) if f.endswith('.jpg') or f.endswith('.png')]\n    \n#     # Vérifier que la liste des images n'est pas vide\n#     if not image_files:\n#         print(f\"Aucune image trouvée dans le dossier {input_folder}\")\n#         return\n   \n#     for image_file in tqdm(image_files, desc=\"Conversion des images en noir sur blanc\"):\n#         input_path = os.path.join(input_folder, image_file)\n#         output_path = os.path.join(output_folder, image_file)\n        \n#         # Charger l'image actuelle\n#         image = cv2.imread(input_path)\n#         if image is None:\n#             print(f\"Image non trouvée ou non lisible : {input_path}\")\n#             continue\n        \n#         # Convertir en noir sur blanc\n#         bw_image = convert_to_black_on_white(image)\n        \n#         # Sauvegarder l'image convertie\n#         success = cv2.imwrite(output_path, bw_image)\n#         if not success:\n#             print(f\"Erreur lors de la sauvegarde de l'image : {output_path}\")\n\n# # Utilisation\n# batch_process_images(TRAIN_IMAGES_DIR, TRAIN_IMAGES_DIR)\n# batch_process_images(VAL_IMAGES_DIR, VAL_IMAGES_DIR)\n# batch_process_images(TEST_IMAGES_DIR, TEST_IMAGES_DIR)\nX_train.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# import cv2\n# import os\n# import numpy as np\n# from tqdm import tqdm\n# from skimage.exposure import match_histograms\n\n# def match_histogram(image, reference):\n#     # Convertir les images BGR en RGB (OpenCV utilise BGR par défaut)\n#     image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n#     reference_rgb = cv2.cvtColor(reference, cv2.COLOR_BGR2RGB)\n    \n#     # Appliquer la correspondance d'histogramme avec channel_axis au lieu de multichannel\n#     matched = match_histograms(image_rgb, reference_rgb, channel_axis=-1)\n    \n#     # Reconvertir l'image correspondante en BGR pour OpenCV\n#     matched_bgr = cv2.cvtColor((matched * 255).astype(np.uint8), cv2.COLOR_RGB2BGR)\n#     return matched_bgr\n\n# def batch_match_histograms(input_folder, output_folder, reference_image_path):\n#     os.makedirs(output_folder, exist_ok=True)\n    \n#     # Charger l'image de référence\n#     reference_image = cv2.imread(reference_image_path)\n#     if reference_image is None:\n#         raise ValueError(f\"Erreur : L'image de référence n'a pas été trouvée au chemin {reference_image_path}\")\n    \n#     # print(f\"Image de référence chargée avec succès : {reference_image_path}\")\n\n#     # Obtenir la liste des fichiers d'images\n#     image_files = [f for f in os.listdir(input_folder) if f.endswith('.jpg') or f.endswith('.png')]\n#     # Vérifier que la liste des images n'est pas vide\n#     if not image_files:\n#         print(f\"Aucune image trouvée dans le dossier {input_folder}\")\n#         return\n   \n#     for image_file in tqdm(image_files, desc=\"Normalisation des images par correspondance d'histogramme\"):\n#         input_path = os.path.join(input_folder, image_file)\n#         output_path = os.path.join(output_folder, image_file)\n        \n#         # Charger l'image actuelle\n#         image = cv2.imread(input_path)\n#         if image is None:\n#             print(f\"Image non trouvée ou non lisible : {input_path}\")\n#             continue\n        \n#         # Appliquer la correspondance d'histogramme\n#         normalized_image = match_histogram(image, reference_image)\n        \n#         # Sauvegarder l'image normalisée\n#         success = cv2.imwrite(output_path, normalized_image)\n#         # if success:\n#         #     print(f\"Image normalisée sauvegardée : {output_path}\")\n#         # else:\n#         #     print(f\"Erreur lors de la sauvegarde de l'image : {output_path}\")\n        \n#         # # Supprimer l'image originale\n#         # try:\n#         #     os.remove(input_path)\n#         #     print(f\"Image originale supprimée : {input_path}\")\n#         # except Exception as e:\n#         #     print(f\"Erreur lors de la suppression de l'image originale {input_path}: {e}\")\n\n# # Utilisation\n# reference_image_path = '../input/id_2fzsen7crd.jpg'  # Chemin vers l'image de référence\n\n# batch_match_histograms(TRAIN_IMAGES_DIR, TRAIN_IMAGES_DIR, reference_image_path)\n# batch_match_histograms(VAL_IMAGES_DIR, VAL_IMAGES_DIR, reference_image_path)\n# batch_match_histograms(TEST_IMAGES_DIR, TEST_IMAGES_DIR, reference_image_path)\n\n#--------------------------------------------------------------------------------\n\n# import torch\n# import cv2\n# import numpy as np\n# import pandas as pd\n# import albumentations as A\n# import os\n# from tqdm import tqdm\n# import matplotlib.pyplot as plt\n\n# # Répertoire de sauvegarde des images augmentées\n# AUGMENTED_IMAGES_DIR = TRAIN_IMAGES_DIR\n# os.makedirs(AUGMENTED_IMAGES_DIR, exist_ok=True)  # Crée le dossier si non existant\n\n# # Fonction pour ajuster la luminosité à une valeur fixe\n# def set_fixed_brightness(image, brightness_value=100):\n#     image = np.clip(image + brightness_value, 0, 255).astype(np.uint8)\n#     return image\n\n# # Fonction pour ajuster l'exposition (similaire à la luminosité)\n# def set_fixed_exposure(image, exposure_value=100):\n#     image = np.clip(image * (1 + exposure_value / 100), 0, 255).astype(np.uint8)\n#     return image\n\n# # Enveloppe de luminosité et exposition fixes\n# def fixed_brightness_and_exposure(image, **kwargs):\n#     image = set_fixed_brightness(image, brightness_value=100)\n#     return set_fixed_exposure(image, exposure_value=100)\n\n# # Définir les augmentations\n# transform = A.Compose([\n#     A.RandomRotate90(),\n#     A.HorizontalFlip(p=0.5),\n#     A.VerticalFlip(p=0.5),\n#     A.HueSaturationValue(hue_shift_limit=0, sat_shift_limit=(-100, 100), val_shift_limit=(0, 0), p=1.0),\n#     A.Lambda(image=fixed_brightness_and_exposure),  # Appliquer luminosité et exposition fixes\n#     A.RandomBrightnessContrast(p=0.5),  # Ajuster contraste et luminosité de manière aléatoire\n#     A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),  # Ajouter du bruit\n#     #A.Resize(640, 640),\n# ])\n\n# def load_and_augment_image(image_path):\n#     # Vérifier si l'image existe\n#     if not os.path.exists(image_path):\n#         print(f\"Image d'origine introuvable : {image_path}\")\n#         return None\n    \n#     # Charger l'image\n#     image = cv2.imread(image_path)\n#     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convertir BGR en RGB\n#     image = image.astype(np.uint8)  # S'assurer que l'image est en uint8 pour Albumentations\n\n#     # Appliquer les augmentations\n#     augmented = transform(image=image)\n#     augmented_image = augmented['image']\n    \n#     return augmented_image\n\n# # Fonction pour augmenter le dataset\n# def augment_dataset(X_train):\n#     # Compter le nombre d'instances pour chaque class_id\n#     class_counts = X_train['class_id'].value_counts()\n\n#     # Trouver la classe minoritaire et majoritaire\n#     class_minority = class_counts.idxmin()\n#     class_majority = class_counts.idxmax()\n#     count_minority = class_counts.min()\n#     count_majority = class_counts.max()\n\n#     # Augmenter la classe minoritaire\n#     augmented_images = []\n#     augmented_records = []\n\n#     while class_counts[class_minority] < count_majority:\n#         for index, row in X_train.iterrows():\n#             if row['class_id'] == class_minority:\n#                 # Charger et augmenter l'image\n#                 augmented_image = load_and_augment_image(row['image_path'])\n                \n#                 if augmented_image is not None:\n#                     # Générer un nom unique pour l'image augmentée\n#                     filename, file_extension = os.path.splitext(os.path.basename(row['image_path']))\n#                     augmented_image_path = os.path.join(AUGMENTED_IMAGES_DIR, f\"{filename}_aug{len(augmented_images)}{file_extension}\")\n\n#                     # Enregistrer l'image augmentée\n#                     cv2.imwrite(augmented_image_path, cv2.cvtColor(augmented_image, cv2.COLOR_RGB2BGR))\n                    \n# #                     # Vérifier que l'image est bien sauvegardée\n# #                     if os.path.exists(augmented_image_path):\n# #                         print(f\"Image augmentée sauvegardée : {augmented_image_path}\")\n# #                     else:\n# #                         print(f\"Échec de la sauvegarde pour l'image : {augmented_image_path}\")\n\n#                     # Créer un enregistrement pour le DataFrame augmenté\n#                     augmented_record = row.copy()\n#                     augmented_record['Image_ID'] = f\"{filename}_aug{len(augmented_images)}{file_extension}\"\n#                     augmented_record['image_path'] = augmented_image_path\n#                     augmented_records.append(augmented_record)\n\n#                     # Mettre à jour le compteur pour la classe minoritaire\n#                     class_counts[class_minority] += 1\n#                     if class_counts[class_minority] >= count_majority:\n#                         break\n\n#     # Créer un DataFrame pour les images augmentées\n#     augmented_df = pd.DataFrame(augmented_records)\n    \n#     augmented_df.head()\n#     # Concaténer le DataFrame original avec le DataFrame des augmentations\n#     X_train = pd.concat([X_train, augmented_df], ignore_index=True)\n\n#     return X_train\n\n# # Charger le DataFrame et effectuer l'augmentation\n# X_train = augment_dataset(X_train)\n\n# # Afficher quelques informations après l'augmentation\n# print(f\"Nombre total d'images dans le DataFrame après augmentation : {len(X_train)}\")\n# print(f\"Images augmentées dans le DataFrame : {X_train.tail(10)[['Image_ID', 'image_path']]}\")\n\n# # Vérification visuelle des images augmentées\n# print(\"Affichage de quelques images augmentées :\")\n# for img_path in os.listdir(AUGMENTED_IMAGES_DIR)[:5]:  # Affiche seulement les 5 premières images augmentées pour validation\n#     full_path = os.path.join(AUGMENTED_IMAGES_DIR, img_path)\n#     if os.path.exists(full_path):\n#         img = cv2.imread(full_path)\n#         plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n#         plt.title(f\"Image augmentée: {img_path}\")\n#         plt.show()\n#     else:\n#         print(f\"Image augmentée introuvable : {full_path}\")","metadata":{"_uuid":"6bd51a69-71db-4e32-8809-d7cb26ed6271","_cell_guid":"19e1d35b-4ff5-4d48-903e-ac39baa7eee1","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport multiprocessing\nfrom tqdm import tqdm\n\n# Fonction pour récupérer les dimensions d'une image\ndef get_image_dimensions(image_path):\n    try:\n        with Image.open(image_path) as img:\n            return img.width, img.height\n    except Exception as e:\n        print(f\"Erreur lors de l'ouverture de l'image {image_path}: {e}\")\n        return None, None\n\n# Fonction pour créer une annotation YOLO pour une image spécifique\ndef save_yolo_annotation(row):\n    image_id = row['Image_ID']\n    image_path = row['image_path']\n    output_dir = row['output_dir']\n    \n    # Récupérer les dimensions de l'image\n    image_width, image_height = get_image_dimensions(image_path)\n    \n    # Vérifier si l'image a bien été chargée\n    if image_width is None or image_height is None:\n        return f\"Erreur: Impossible d'ouvrir l'image {image_path}\"\n\n    # Définir le chemin du fichier d'annotation\n    annotation_file_path = os.path.join(output_dir, f\"{image_id.split('.')[0]}.txt\")\n    os.makedirs(output_dir, exist_ok=True)  # S'assurer que le dossier existe\n    \n    # Calcul des coordonnées normalisées (format YOLO)\n    try:\n        x_center = (row['xmin'] + row['xmax']) / 2 / image_width\n        y_center = (row['ymin'] + row['ymax']) / 2 / image_height\n        width = (row['xmax'] - row['xmin']) / image_width\n        height = (row['ymax'] - row['ymin']) / image_height\n        \n        # Sauvegarder l'annotation dans le fichier\n        with open(annotation_file_path, 'a') as f:\n            f.write(f\"{row['class_id']} {x_center} {y_center} {width} {height}\\n\")\n        \n        return f\"Annotation sauvegardée dans {annotation_file_path}\"\n    \n    except Exception as e:\n        return f\"Erreur lors de la sauvegarde de l'annotation pour {image_id}: {e}\"\n\n# Fonction pour paralléliser la création des annotations pour un DataFrame\ndef create_annotations(df, base_output_annotation_folder):\n    # Utilisation de multiprocessing pour traiter chaque ligne du DataFrame en parallèle\n    with multiprocessing.Pool() as pool:\n        results = []\n        for _, row in tqdm(df.iterrows(), total=df.shape[0]):\n            # Déterminer le répertoire de sortie en fonction de la classe\n            class_id = row['class_id']\n            class_dir = f'class_{class_id}'\n            output_annotation_folder = os.path.join(base_output_annotation_folder, class_dir)\n            \n            # Ajouter cette information au DataFrame\n            row['output_dir'] = output_annotation_folder\n            \n            # Appeler la fonction de création des annotations\n            results.append(pool.apply_async(save_yolo_annotation, (row,)))\n        \n        # Attendre la fin du traitement\n        for result in results:\n            result.get()  # Récupérer les résultats pour s'assurer que tout est traité\n\n# Créer les annotations pour les ensembles d'entraînement et de validation\ncreate_annotations(X_train, TRAIN_LABELS_DIR)\ncreate_annotations(X_val, VAL_LABELS_DIR)\n","metadata":{"_uuid":"8d7d8967-3dfe-4760-b0a6-0807ad02a079","_cell_guid":"00da02dd-0719-4e50-a69d-6e1ff98e83f2","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# from PIL import Image\n# import multiprocessing\n# from tqdm import tqdm\n\n# # Fonction pour récupérer les dimensions d'une image\n# def get_image_dimensions(image_path):\n#     try:\n#         with Image.open(image_path) as img:\n#             return img.width, img.height\n#     except Exception as e:\n#         print(f\"Erreur lors de l'ouverture de l'image {image_path}: {e}\")\n#         return None, None\n\n# # Fonction pour créer une annotation YOLO pour une image spécifique\n# def save_yolo_annotation(row):\n#     image_id = row['Image_ID']\n#     image_path = row['image_path']\n#     output_dir = row['output_dir']\n    \n#     # Récupérer les dimensions de l'image\n#     image_width, image_height = get_image_dimensions(image_path)\n    \n#     # Vérifier si l'image a bien été chargée\n#     if image_width is None or image_height is None:\n#         return f\"Erreur: Impossible d'ouvrir l'image {image_path}\"\n\n#     # Définir le chemin du fichier d'annotation\n#     annotation_file_path = os.path.join(output_dir, f\"{image_id.split('.')[0]}.txt\")\n#     os.makedirs(output_dir, exist_ok=True)  # S'assurer que le dossier existe\n    \n#     # Calcul des coordonnées normalisées (format YOLO)\n#     try:\n#         x_center = (row['xmin'] + row['xmax']) / 2 / image_width\n#         y_center = (row['ymin'] + row['ymax']) / 2 / image_height\n#         width = (row['xmax'] - row['xmin']) / image_width\n#         height = (row['ymax'] - row['ymin']) / image_height\n        \n#         # Sauvegarder l'annotation dans le fichier\n#         with open(annotation_file_path, 'a') as f:\n#             f.write(f\"{row['class_id']} {x_center} {y_center} {width} {height}\\n\")\n        \n#         return f\"Annotation sauvegardée dans {annotation_file_path}\"\n    \n#     except Exception as e:\n#         return f\"Erreur lors de la sauvegarde de l'annotation pour {image_id}: {e}\"\n\n# # Fonction pour paralléliser la création des annotations pour un DataFrame\n# def create_annotations(df, output_annotation_folder):\n#     # Ajoute une colonne 'output_dir' au DataFrame\n#     df['output_dir'] = output_annotation_folder\n    \n#     # Utilisation de multiprocessing pour traiter chaque ligne du DataFrame en parallèle\n#     with multiprocessing.Pool() as pool:\n#         results = list(tqdm(pool.imap(save_yolo_annotation, df.to_dict('records')), total=len(df)))\n    \n#     # # Affichage des résultats\n#     # for result in results:\n#     #     print(result)\n\n# # Créer les annotations pour les ensembles d'entraînement et de validation\n# create_annotations(X_train, TRAIN_LABELS_DIR)\n# create_annotations(X_val, VAL_LABELS_DIR)\n","metadata":{"_uuid":"69aa4a76-8269-4985-80d6-c42ec0f22b23","_cell_guid":"ab72d11e-eba8-4853-a98e-c8264beeb0c5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"71b636af-f33b-4788-af1a-0ac4087b1863","_cell_guid":"19dd99e7-5be4-4d37-ac05-732842587ae0","collapsed":false,"id":"3Az4Eh_MhPyq","outputId":"3aa73f7a-778a-434a-c5a5-b956d37d97de","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#normalisation des images avec pytorch\n# import torch\n# from torchvision import datasets, transforms\n# import torch.utils.data\n\n# # Fonction pour calculer la moyenne et l'écart-type\n# def get_mean_std(loader):\n#     num_pixels = 0\n#     mean = 0.0\n#     std = 0.0\n#     for images, _ in loader:\n#         batch_size, num_channels, height, width = images.shape\n#         num_pixels += batch_size * height * width\n#         mean += images.mean(axis=(0, 2, 3)).sum()\n#         std += images.std(axis=(0, 2, 3)).sum()\n\n#     mean /= num_pixels\n#     std /= num_pixels\n#     return mean, std\n\n# # Charger ton jeu de données d'entraînement avec redimensionnement et transformation\n# data_dir = TRAIN_IMAGES_DIR\n# transform = transforms.Compose([\n#     transforms.Resize((640, 640)),  # Redimensionner les images à 256x256\n#     transforms.ToTensor()\n# ])\n# dataset = datasets.ImageFolder(data_dir, transform=transform)\n\n# # Créer un DataLoader pour parcourir les images\n# batch_size = 32\n# loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n# # Calculer la moyenne et l'écart-type\n# mean, std = get_mean_std(loader)\n# print(f\"Mean: {mean}, Std: {std}\")\n","metadata":{"_uuid":"9f661a4e-2864-4958-9b53-3871d97cc672","_cell_guid":"40d1f713-644a-4a8e-aef8-4c8b442c1bcb","collapsed":false,"id":"k6ka7dY4iH1U","outputId":"c4a11ec7-13d9-4643-c4e8-068179132b3f","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from torchvision import transforms\n\n# # Appliquer la normalisation avec la moyenne et l'écart-type calculées\n# data_transforms = transforms.Compose([\n#     transforms.Resize((640, 640)),  # Redimensionner l'image à une taille fixe pour YOLO\n#     transforms.ToTensor(),\n#     transforms.Normalize(mean=mean, std=std)  # Normaliser\n# ])\n\n# # Charger l'ensemble de données avec transformation\n# train_dataset = datasets.ImageFolder(data_dir, transform=data_transforms)\n# val_dataset = datasets.ImageFolder(VAL_IMAGES_DIR, transform=data_transforms)\n\n# # DataLoader pour entraîner et valider le modèle\n# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n# val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False)\n","metadata":{"_uuid":"0ce02bf2-6791-4d47-8775-e26390fa22a3","_cell_guid":"e5b1278d-d33f-463a-a4c2-954a82acbc7f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport os\nfrom pathlib import Path\nimport numpy as np\n\n\ndef normalize_image_for_yolo(image, size=(416, 416)):\n    \"\"\"\n    Normalise une image pour un modèle YOLO en utilisant cv2.dnn.blobFromImage.\n    \"\"\"\n    blob = cv2.dnn.blobFromImage(image, scalefactor=1/255.0, size=size, swapRB=True, crop=False)\n    normalized_image = blob[0]  # Extraire le premier (et unique) blob\n    return normalized_image\n\ndef process_images_in_directory(input_dir, output_size=(416, 416)):\n    normalized_images = []\n    for image_file in input_dir.glob(\"*.jpg\"):\n        image = cv2.imread(str(image_file))\n        if image is not None:\n            normalized_image = normalize_image_for_yolo(image, size=output_size)\n            normalized_images.append(normalized_image)\n        else:\n            print(f\"Image non trouvée ou non lisible : {image_file}\")\n    return normalized_images\n\n# Traiter les images d'entraînement et de validation\ntrain_images_normalized = process_images_in_directory(TRAIN_IMAGES_DIR)\nval_images_normalized = process_images_in_directory(VAL_IMAGES_DIR)\n\nprint(f\"Nombre d'images normalisées dans le dossier d'entraînement : {len(train_images_normalized)}\")\nprint(f\"Nombre d'images normalisées dans le dossier de validation : {len(val_images_normalized)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a data.yaml file required by yolo\nclass_names = train['class'].unique().tolist()  # Récupère la liste des noms de classes uniques dans le jeu de données d'entraînement\nnum_classes = len(class_names)                  # Calcule le nombre de classes\n\ndata_yaml = {\n    # getcwd() est une fonction qui retourne une chaîne de caractères représentant le chemin absolu du répertoire de travail actuel\n    'train': os.getcwd()+'/' + str(TRAIN_IMAGES_DIR),    # Définit le chemin du répertoire des images d'entraînement\n    'val': os.getcwd()+'/' + str(VAL_IMAGES_DIR),        # Définit le chemin du répertoire des images de validation\n    'test': os.getcwd()+'/' + str(TEST_IMAGES_DIR),      # Définit le chemin du répertoire des images de test\n    'nc': num_classes,                                   # Définit le nombre de classes\n    'names': class_names                                 # Définit la liste des noms de classes\n}\n\n\nyaml_path = 'data.yaml'                            # Définit le chemin du fichier YAML à créer\n\nwith open(yaml_path, 'w') as file:\n    yaml.dump(data_yaml, file, default_flow_style=False)            # Écrit le dictionnaire data_yaml dans le fichier YAML\n\n# Preview data yaml file\ndata_yaml                                             # Affiche le contenu du fichier YAML créé","metadata":{"_uuid":"f3691237-2859-41a1-99f0-899c41490e9d","_cell_guid":"9d3c3d04-4114-4050-a935-0ea1bb415cf7","collapsed":false,"id":"mYrPsLMHIyW_","outputId":"88ab3d7c-e323-46f6-f680-429067c940dd","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom tqdm import tqdm\n\n# Fonction pour charger les annotations YOLO à partir d'un fichier\ndef load_annotations(label_path):\n    with open(label_path, 'r') as f:\n        lines = f.readlines()\n    boxes = []\n    for line in lines:\n        class_id, x_center, y_center, width, height = map(float, line.strip().split())\n        boxes.append((class_id, x_center, y_center, width, height))\n    return boxes\n\n# Fonction pour afficher une image avec ses boîtes englobantes\ndef plot_image_with_boxes(image_path, boxes):\n    image = cv2.imread(str(image_path))\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n    h, w, _ = image.shape\n    plt.figure(figsize=(10, 10))\n    plt.imshow(image)\n    \n    for box in boxes:\n        class_id, x_center, y_center, width, height = box\n        xmin = int((x_center - width / 2) * w)\n        ymin = int((y_center - height / 2) * h)\n        xmax = int((x_center + width / 2) * w)\n        ymax = int((y_center + height / 2) * h)\n        \n        plt.gca().add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n                                          edgecolor='red', facecolor='none', linewidth=2))\n        plt.text(xmin, ymin - 10, f'Class {int(class_id)}', color='red', fontsize=12, weight='bold')\n    \n    plt.axis('off')\n    plt.show()\n\n# Chemins de base pour les images et labels\nBASE_DIR = Path(\"dataset\")\nIMAGE_DIR = IMAGES_DIR\nLABEL_DIR = LABELS_DIR\n\n# Extensions d'image possibles\nextensions = ['.jpg', '.png']\n\n# Parcourir les sous-dossiers (train, val, test)\nfor split in ['train', 'val', 'test']:\n    image_split_dir = IMAGE_DIR / split\n    label_split_dir = LABEL_DIR / split\n    \n    # Parcourir les classes (class_0, class_1)\n    for class_folder in label_split_dir.iterdir():\n        if class_folder.is_dir():\n            class_id = int(class_folder.name.split('_')[-1])\n            \n            # Parcourir chaque fichier d'annotation dans le dossier de classe\n            for label_file in tqdm(list(class_folder.glob(\"*.txt\"))[:3], desc=f\"Plotting images for {split}/{class_folder.name}\"):\n                image_name = label_file.stem  # Nom sans extension\n                image_path = None\n                \n                # Recherchez l'image avec l'une des extensions possibles dans le dossier images\n                for ext in extensions:\n                    potential_path = image_split_dir / class_folder.name / f\"{image_name}{ext}\"\n                    if potential_path.exists():\n                        image_path = potential_path\n                        break\n                \n                if image_path:\n                    boxes = load_annotations(label_file)\n                    print(f\"Plotting {image_path.name} with {len(boxes)} bounding boxes.\")\n                    plot_image_with_boxes(image_path, boxes)\n                else:\n                    print(f\"No image found for annotation {label_file.name} in {split}/{class_folder.name}.\")\n","metadata":{"_uuid":"771b33e2-63db-44f4-95d1-7eae5dc68e2c","_cell_guid":"58d4655a-a57d-41e4-bf1d-a6e927d7ab74","collapsed":false,"id":"h3LwCmC9xusu","outputId":"1133effa-dfc9-4e91-ec1b-2044110c0e63","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Récupération sécurisée de la clé API de Weights & Biases (WandB)\n\nuser_secrets = UserSecretsClient()\nwandb_api_key = user_secrets.get_secret(\"yolo_key\")\n\n# Définition de la clé API dans les variables d'environnement pour WandB\nos.environ['WANDB_API_KEY'] = wandb_api_key","metadata":{"_uuid":"56deed1e-6ad4-4edc-be91-09ef3ecdf950","_cell_guid":"1688eab4-e33d-4e23-a2de-f8b5c38508e4","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# train_directory = TRAIN_IMAGES_DIR\n\n# from skopt import BayesSearchCV\n# from skopt.space import Real, Integer, Categorical\n# from sklearn.base import BaseEstimator\n# from ultralytics import YOLO\n# import os\n# import numpy as np\n\n# class YOLOEstimator(BaseEstimator):\n#     def __init__(self, model_path='/kaggle/input/yolo_fine_8l/tensorflow2/default/1/best.pt', data_yaml='data.yaml'):\n#         self.model_path = model_path\n#         self.data_yaml = data_yaml\n#         self.model = None\n\n#     def fit(self, X, y=None, **params):\n#         self.model = YOLO(self.model_path)\n        \n#         # Entraînement avec les hyperparamètres fournis\n#         train_args = {\n\n  \n#             'data': self.data_yaml,\n#             'epochs': int(params.get('epochs', 25)),\n#             'imgsz': int(params.get('imgsz', 640)),\n#             'batch': int(params.get('batch', 32)),\n#             'device': params.get('device', [0, 1]),\n#             'lr0': float(params.get('lr0', 0.0017399892628386226)),\n#             'lrf': float(params.get('lrf', 0.01)),\n#             'momentum': float(params.get('momentum', 0.8369417969048814)),\n#             'weight_decay': float(params.get('weight_decay', 0.0005607011624693186)),\n#         }\n        \n        \n#         try:\n#             results = self.model.train(**train_args)\n#             self.best_map = results.maps.get('mAP50-95(B)', 0)\n#         except Exception as e:\n#             print(f\"Erreur lors de l'entraînement : {e}\")\n#             self.best_map = 0\n        \n#         return self\n\n#     def score(self, X, y=None):\n#         if self.model is None:\n#             return 0\n#         try:\n#             results = self.model.val()\n#             return results.maps.get('mAP50-95(B)', 0)\n#         except Exception as e:\n#             print(f\"Erreur lors de la validation : {e}\")\n#             return 0\n\n#     def get_params(self, deep=True):\n#         return {\n#             'model_path': self.model_path,\n#             'data_yaml': self.data_yaml\n#         }\n\n#     def set_params(self, **params):\n#         for key, value in params.items():\n#             setattr(self, key, value)\n#         return self\n\n# # Définition de l'espace de recherche pour chaque hyperparamètre à optimiser\n# search_space = {\n#     'epochs': Integer(5, 50),  # Nombre d'époques ajusté\n#     'imgsz': Integer(400, 1024),  # Taille des images d'entrée\n#     'batch': Integer(16, 32),  # Taille du lot ajustée\n#     'lr0': Real(1e-4, 1e-2),          # Taux d'apprentissage initial\n#     'weight_decay': Real(1e-5, 1e-3), # Décroissance des poids\n#     'momentum': Real(0.7, 0.95),       # Momentum\n#     'lrf':  Real(1e-4, 1e-2)      \n# }\n\n\n\n# # Nombre total d'itérations pour la recherche bayesienne\n# n_iterations = 10\n\n\n# # Récupérer les chemins d'images du dossier d'entraînement\n# X_train = [os.path.join(train_directory, img) for img in os.listdir(train_directory) if img.endswith(('.jpg', '.jpeg', '.png'))]\n# y_train = None  # Pas besoin d'utiliser y_train ici car nous utilisons les chemins directement\n\n# # Entraînement avec YOLOv9 et recherche bayesienne\n# optimizer = BayesSearchCV(\n#     estimator=YOLOEstimator(model_path='/kaggle/input/yolo_fine_8l/tensorflow2/default/1/best.pt', data_yaml='data.yaml'),\n#     search_spaces=search_space,\n#     n_iter=n_iterations,  # Nombre d'itérations pour la recherche bayesienne\n#     cv=3,  # Utiliser une validation croisée\n#     n_jobs=-1,  # Utilisation de tous les processeurs disponibles\n#     verbose=False\n# )\n\n# # Ajustement du modèle avec le jeu de données d'entraînement\n# optimizer.fit(X_train, y_train)\n\n# # Meilleurs hyperparamètres trouvés\n# best_hyperparams = optimizer.best_params_\n# print(\"Meilleurs hyperparamètres trouvés : \", best_hyperparams)\n\n\n\n\n\n# # def objective(trial):\n# #     # Définir les hyperparamètres à optimiser avec Optuna\n# #     train_args = {\n# #             'data': 'data.yaml',\n# #             'epochs': 5,\n# #             'patience': 10,\n# #             'batch': 16,\n# #             'imgsz': 320,\n# #             'save': True,\n# #             'save_period': -1,\n# #             'cache': False,\n# #             'device': [0,1],  \n# #             'workers': 8,\n# #             'project': \"optuna_trials\",\n# #             'name': f\"trial_{trial.number}\",\n# #             'optimizer': trial.suggest_categorical('optimizer', ['SGD', 'Adam', 'AdamW']),\n# #             'cos_lr': trial.suggest_categorical('cos_lr', [True, False]),\n# #             'rect': trial.suggest_categorical('rect', [True, False]),\n# #             'amp': trial.suggest_categorical('amp', [True, False]),\n# #             'multi_scale': trial.suggest_categorical('multi_scale', [True, False]),\n# #             'lr0': trial.suggest_loguniform('lr0', 1e-4, 1e-2),\n# #             'lrf': trial.suggest_uniform('lrf', 0.01, 0.3),\n# #             'momentum': trial.suggest_uniform('momentum', 0.85, 0.99),\n# #             'weight_decay': trial.suggest_loguniform('weight_decay', 1e-5, 1e-3),\n# #             'warmup_epochs': trial.suggest_uniform('warmup_epochs', 0, 5),\n# #             'warmup_momentum': trial.suggest_uniform('warmup_momentum', 0.7, 0.9),\n# #             'warmup_bias_lr': trial.suggest_uniform('warmup_bias_lr', 0.05, 0.2),\n# #             'box': trial.suggest_uniform('box', 0.02, 0.2),\n# #             'cls': trial.suggest_uniform('cls', 0.2, 4.0),\n# #             'dfl': trial.suggest_uniform('dfl', 0.5, 2.0),\n# #             'pose': trial.suggest_uniform('pose', 1.0, 20.0),\n# #             'kobj': trial.suggest_uniform('kobj', 0.1, 2.0),\n# #             'label_smoothing': trial.suggest_uniform('label_smoothing', 0.0, 0.1),\n# #             'hsv_h': trial.suggest_uniform('hsv_h', 0.0, 0.1),\n# #             'hsv_s': trial.suggest_uniform('hsv_s', 0.5, 1.0),\n# #             'hsv_v': trial.suggest_uniform('hsv_v', 0.2, 1.0),\n# #             'degrees': trial.suggest_uniform('degrees', 0, 5),\n# #             'translate': trial.suggest_uniform('translate', 0, 0.5),\n# #             'scale': trial.suggest_uniform('scale', 0.0, 1.0),\n# #             'shear': trial.suggest_uniform('shear', 0.0, 5.0),\n# #             'perspective': trial.suggest_uniform('perspective', 0.0, 0.001),\n# #             'flipud': trial.suggest_uniform('flipud', 0.0, 0.5),\n# #             'fliplr': trial.suggest_uniform('fliplr', 0.0, 0.5),\n# #             'mosaic': trial.suggest_uniform('mosaic', 0.0, 1.0),\n# #             'mixup': trial.suggest_uniform('mixup', 0.0, 1.0),\n# #             'copy_paste': trial.suggest_uniform('copy_paste', 0.0, 0.5)\n# #         }\n\n# #         # Configurer et entraîner le modèle YOLO\n# #     model = YOLO(\"/kaggle/input/yolo_fine_8l/tensorflow2/default/1/best.pt\")  # Assurez-vous que le fichier modèle est compatible\n# #     results = model.train(**train_args)\n# #     result = model.val()\n# #         # Définir la métrique à optimiser (ex: mAP@0.5)\n# #     map_50 = result.box.map50  # mAP@0.5\n    \n# #     return map_50\n\n# #     # Lancer l'optimisation avec Optuna\n# # study = optuna.create_study(direction=\"maximize\")\n# # study.optimize(objective, n_trials=10)\n\n# #     # Meilleure configuration d'hyperparamètres\n# # print(\"Best hyperparameters:\", study.best_params)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import random\n# from deap import base, creator, tools, algorithms\n# from ultralytics import YOLO\n\n# # Fonction pour évaluer le modèle avec les hyperparamètres\n# def evaluate(individual):\n#     # Extraire les valeurs des individus\n#     (optimizer, cos_lr, rect, amp, multi_scale, lr0, lrf, momentum, weight_decay, warmup_epochs,\n#      warmup_momentum, warmup_bias_lr, box, cls, dfl, pose, kobj, label_smoothing,\n#      hsv_h, hsv_s, hsv_v, degrees, translate, scale, shear, perspective, flipud,\n#      fliplr, mosaic, mixup, copy_paste) = individual\n\n#     # Configurer YOLO avec les hyperparamètres\n#     model = YOLO(\"yolov8l.pt\")\n#     train_args = {\n#         'data': 'data.yaml',\n#         'epochs': 10,\n#         'patience': 5,\n#         'batch': 16,\n#         'imgsz': 340,\n#         'save': True,\n#         'optimizer': optimizer,\n#         'cos_lr': cos_lr,\n#         'rect': rect,\n#         'amp': amp,\n#         'multi_scale': multi_scale,\n#         'lr0': lr0,\n#         'lrf': lrf,\n#         'momentum': momentum,\n#         'weight_decay': weight_decay,\n#         'warmup_epochs': warmup_epochs,\n#         'warmup_momentum': warmup_momentum,\n#         'warmup_bias_lr': warmup_bias_lr,\n#         'box': box,\n#         'cls': cls,\n#         'dfl': dfl,\n#         'pose': pose,\n#         'kobj': kobj,\n#         'label_smoothing': label_smoothing,\n#         'hsv_h': hsv_h,\n#         'hsv_s': hsv_s,\n#         'hsv_v': hsv_v,\n#         'degrees': degrees,\n#         'translate': translate,\n#         'scale': scale,\n#         'shear': shear,\n#         'perspective': perspective,\n#         'flipud': flipud,\n#         'fliplr': fliplr,\n#         'mosaic': mosaic,\n#         'mixup': mixup,\n#         'copy_paste': copy_paste,\n#         'device': [0,1],\n#     }\n\n#     # Entraîner le modèle\n#     results = model.train(**train_args)\n    \n#     result = model.val()\n#     # Définir la métrique à optimiser (ex: mAP@0.5)\n#     map_50 = result.box.map50  # mAP@0.5\n#     return map_50,\n\n# # Supprimer les classes existantes\n# if \"FitnessMax\" in creator.__dict__:\n#     del creator.FitnessMax\n# if \"Individual\" in creator.__dict__:\n#     del creator.Individual\n\n# # Initialisation de DEAP\n# creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n# creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n\n# toolbox = base.Toolbox()\n# toolbox.register(\"optimizer\", random.choice, [\"SGD\", \"Adam\", \"AdamW\"])\n# toolbox.register(\"cos_lr\", random.choice, [True, False])\n# toolbox.register(\"rect\", random.choice, [True, False])\n# toolbox.register(\"amp\", random.choice, [True, False])\n# toolbox.register(\"multi_scale\", random.choice, [True, False])\n# toolbox.register(\"lr0\", random.uniform, 1e-4, 1e-2)\n# toolbox.register(\"lrf\", random.uniform, 0.0, 1.0)\n# toolbox.register(\"momentum\", random.uniform, 0.0, 1.0)\n# toolbox.register(\"weight_decay\", random.uniform, 1e-5, 1e-3)\n# toolbox.register(\"warmup_epochs\", random.uniform, 0, 5)\n# toolbox.register(\"warmup_momentum\", random.uniform, 0.0, 1.0)\n# toolbox.register(\"warmup_bias_lr\", random.uniform, 0.0, 1.0)\n# toolbox.register(\"box\", random.uniform, 0.0, 1.0)\n# toolbox.register(\"cls\", random.uniform, 0.0, 1.0)\n# toolbox.register(\"dfl\", random.uniform, 0.0, 1.0)\n# toolbox.register(\"pose\", random.uniform, 1.0, 20.0)\n# toolbox.register(\"kobj\", random.uniform, 0.0, 1.0)\n# toolbox.register(\"label_smoothing\", random.uniform, 0.0, 0.1)\n# toolbox.register(\"hsv_h\", random.uniform, 0.0 , 1.0)\n# toolbox.register(\"hsv_s\", random.uniform, 0.0, 1.0)\n# toolbox.register(\"hsv_v\", random.uniform, 0.0, 1.0)\n# toolbox.register(\"degrees\", random.uniform, -180, 180)\n# toolbox.register(\"translate\", random.uniform, 0.0, 1.0)\n# toolbox.register(\"scale\", random.uniform, 0.0, 1.0)\n# toolbox.register(\"shear\", random.uniform, -180, 180)\n# toolbox.register(\"perspective\", random.uniform, 0.0 , 1.0)\n# toolbox.register(\"flipud\", random.choice, [True, False])\n# toolbox.register(\"fliplr\", random.choice, [True, False])\n# toolbox.register(\"mosaic\", random.uniform, 0.0, 1.0)\n# toolbox.register(\"mixup\", random.uniform, 0.0, 1.0)\n# toolbox.register(\"copy_paste\", random.uniform, 0.0, 1.0)\n\n# toolbox.register(\"individual\", tools.initCycle, creator.Individual,\n#                  (toolbox.optimizer, toolbox.cos_lr, toolbox.rect, toolbox.amp, toolbox.multi_scale,\n#                   toolbox.lr0, toolbox.lrf, toolbox.momentum, toolbox.weight_decay, toolbox.warmup_epochs,\n#                   toolbox.warmup_momentum, toolbox.warmup_bias_lr, toolbox.box, toolbox.cls, toolbox.dfl,\n#                   toolbox.pose, toolbox.kobj, toolbox.label_smoothing, toolbox.hsv_h, toolbox.hsv_s,\n#                   toolbox.hsv_v, toolbox.degrees, toolbox.translate, toolbox.scale, toolbox.shear,\n#                   toolbox.perspective, toolbox.flipud, toolbox.fliplr, toolbox.mosaic, toolbox.mixup, toolbox.copy_paste), n=1)\n\n# toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n# toolbox.register(\"evaluate\", evaluate)\n\n# def custom_crossover(ind1, ind2):\n#     for i in range(5, len(ind1)):  # Les indices 0 à 4 sont booléens\n#         if random.random() < 0.5:\n#             ind1[i], ind2[i] = ind2[i], ind1[i]\n#     return ind1, ind2\n\n# toolbox.register(\"mate\", custom_crossover)\n\n# def custom_mutate(individual):\n#     for i in range(5, len(individual)):\n#         if random.random() < 0.2:\n#             individual[i] += random.gauss(0, 0.1)\n#             individual[i] = min(max(individual[i], 0.0), 1.0)\n#     return individual,\n\n# toolbox.register(\"mutate\", custom_mutate)\n# toolbox.register(\"select\", tools.selTournament, tournsize=3)\n\n# def genetic_algorithm():\n#     population = toolbox.population(n=10)\n#     ngen = 5\n#     cxpb, mutpb = 0.5, 0.2\n#     for gen in range(ngen):\n#         offspring = algorithms.varAnd(population, toolbox, cxpb, mutpb)\n#         fits = map(toolbox.evaluate, offspring)\n#         for fit, ind in zip(fits, offspring):\n#             ind.fitness.values = fit\n#         population = toolbox.select(offspring, k=len(population))\n#     best_ind = tools.selBest(population, 1)[0]\n#     print(\"Best hyperparameters:\", best_ind)\n\n# genetic_algorithm()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Chargement des modèles YOLO pré-entraînés\n\nmodel = YOLO(\"yolov8l.pt\")  # charge un modèle YOLO pré-entrainé a partir du fichier","metadata":{"_uuid":"b072b76d-59e1-4d97-b6d8-0b5cb66eacd9","_cell_guid":"eeb31d2b-77dc-48ac-aa86-7d90433171b4","collapsed":false,"id":"KXzxYJYdExAG","outputId":"b5a74a4c-33af-4126-f9ec-168b5f3fbe9b","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Afficher des informations sur le modèle YOLOv9\nmodel.info()","metadata":{"_uuid":"62fbcd97-6456-4e5e-a053-479e23698986","_cell_guid":"f1ef8c80-7fa8-4bb8-95cd-aec005716e05","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Hyperparamètres avec les valeurs respectives\n# parameters = {\n#     'data': 'data.yaml',\n#     'epochs': 10,\n#     'patience': 5,\n#     'batch': 16,\n#     'imgsz': 640,\n#     'optimizer': 'SGD',\n#     'cos_lr': True,\n#     'rect': False,\n#     'amp': True,\n#     'multi_scale': False,\n#     'lr0': 0.005551929282120419,\n#     'lrf': 0.4579592312901949,\n#     'momentum': 0.44236721090471254,\n#     'weight_decay': 0.0009518886980375375,\n#     'warmup_epochs': 3.9408752317959497,\n#     'warmup_momentum': 0.09402580543150785,\n#     'warmup_bias_lr': 0.9768579038881372,\n#     'box': 0.9612251516116247,\n#     'cls': 0.5467338228386568,\n#     'dfl': 0.5583251507678315,\n#     'pose': 6.696711378447303,\n#     'kobj': 0.16850162671180136,\n#     'label_smoothing': 0.04103172349468214,\n#     'hsv_h': 0.9455029840216489,\n#     'hsv_s': 0.222983704358233,\n#     'hsv_v': 0.19951589617132215,\n#     'degrees': -115.19784272109167,\n#     'translate': 0.0244370204819222,\n#     'scale': 0.12042183595275147,\n#     'shear': -48.34816491677475,\n#     'perspective': 0.12116574738152142,\n#     'flipud': True,\n#     'fliplr': True,\n#     'mosaic': 0.4704951902504716,\n#     'mixup': 0.5949810652131459,\n#     'copy_paste': 0.04963149347304907,\n#     'device': [0, 1] \n# }\n\n# model.train(**parameters)","metadata":{"_uuid":"adb868ad-ba17-4c11-bce5-45184c1b0746","_cell_guid":"b45f429b-3d26-4ccc-938e-54e2c8662280","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # # Lister tous les chemins des images dans le répertoire d'entraînement\n# # image_paths = list(train_directory.rglob('*.[jJpP][pPnN][gG]'))[:5]  # Limiter à 5 images\n\n\n# # Fonction pour charger une image\n# def load_image_as_tensor(image_path):\n#     # Charger l'image avec OpenCV\n#     image = cv2.imread(image_path)\n    \n#     # Convertir l'image de BGR (par défaut dans OpenCV) à RGB\n#     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n#     # Appliquer les augmentations\n#     augmented = transform(image=image)\n#     augmented_image = augmented[\"image\"]\n    \n#     return augmented_image\n# # Fonction pour afficher les images\n# def display_images(image_paths):\n#     plt.figure(figsize=(640, 640))  # Ajuster la taille de la figure\n#     for i, img_path in enumerate(image_paths):\n#         image = cv2.imread(str(img_path))\n#         if image is None:\n#             print(f\"Erreur de chargement de l'image : {img_path}\")\n#             continue  # Passer à la prochaine image si celle-ci n'a pas pu être chargée\n#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convertir de BGR à RGB\n        \n#         plt.subplot(1, 5, i + 1)  # Créer une grille de 1 ligne et 5 colonnes\n#         plt.imshow(image)\n#         plt.axis('off')  # Masquer les axes\n#         plt.title(img_path.stem)  # Afficher le nom de l'image sans extension\n\n#     plt.tight_layout()\n#     plt.show()\n\n# # Afficher les cinq premières images\n# #display_images('../input/images/id_fe3rxq4495_aug0.jpg')","metadata":{"_uuid":"8d7a2d9e-dc21-437c-82ff-c3894da39bf0","_cell_guid":"a6e7b40f-5144-4748-8eba-734385adce4f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.train(\n        data         = 'data.yaml',         # Chemin vers les annotations yaml\n        epochs       = 22,              # Nombre d'époques\n        imgsz        = 416,              # Taille des images\n        batch        = 24,               # Taille du lot\n        device       = [0,1],               # Utilisation du GPU\n        lr0          = 0.004282751034955861,\n        lrf          = 0.00274431132496402,\n        momentum     = 0.9409580968621439,\n        weight_decay = 0.0008902799350696401\n    )\n","metadata":{"_uuid":"a29b1999-0120-45ac-83fa-e876f4837a25","_cell_guid":"07664a34-1c55-4f58-a0db-c8947eb1e023","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# # Validate the model on the validation set\n# model = YOLO('/' + os.getcwd() + '/runs/detect/train/weights/best.pt')  # Charge le modèle YOLO à partir du fichier 'best.pt' qui contient les poids du meilleur modèle entraîné\n# results = model.val()      # Évalue le modèle sur le jeu de validation et stocke les résultats de la validation\n\n\n# Vérifier la disponibilité des GPUs\nif torch.cuda.is_available() and torch.cuda.device_count() > 1:\n    print(f\"Nombre de GPUs disponibles : {torch.cuda.device_count()}\")\n    device = [0, 1]  # Utilise les deux premiers GPUs\n    print(f\"Utilisation des GPUs : {device}\")\nelse:\n    print(\"Pas assez de GPUs disponibles, utilisation du CPU ou d'un seul GPU.\")\n    device = [0] if torch.cuda.is_available() else 'cpu'\n\n# Charger le modèle\nmodel_path = '/' + os.getcwd() + '/runs/detect/train/weights/best.pt'\nif os.path.exists(model_path):\n    model = YOLO(model_path)\nelse:\n    raise FileNotFoundError(f\"Le fichier de modèle {model_path} n'existe pas.\")\n\n# Valider le modèle sur l'ensemble de validation\ntry:\n    results = model.val(\n        device=device\n    )\n    \n    # Afficher les résultats\n    \n    # Vous pouvez accéder à des métriques spécifiques comme ceci :\n    \n    print(f\"Precision: {results.box.p}\")\n    print(f\"Recall: {results.box.r}\")\n    # Extraire mAP@0.5 et mAP@[0.5:0.95]\n    map_50 = results.box.map50  # mAP@0.5\n    map_50_95 = results.box.map  # mAP@[0.5:0.95]\n\n    print(f\"mAP@0.5 (VOC): {map_50:.4f}\")\n    print(f\"mAP@[0.5:0.95] (COCO): {map_50_95:.4f}\")\n        \nexcept Exception as e:\n    print(f\"Une erreur s'est produite lors de la validation : {e}\")","metadata":{"_uuid":"ce65d6fa-4d25-4a3d-b41c-0525bef5df87","_cell_guid":"1019f6cb-96a3-40da-9617-68356bc550be","collapsed":false,"id":"PU45G9M6KOrL","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the trained YOLO model  train\nmodel = YOLO('/'+os.getcwd()+'/runs/detect/train/weights/best.pt')\n\n# Path to the test images directory\ntest_dir_path = '/'+os.getcwd()+'/datasets/dataset/images/test'\n\n# Get a list of all image files in the test directory\nimage_files = os.listdir(test_dir_path)  # Récupère la liste de tous les fichiers image dans le répertoire de test\n\n# Initialize an empty list to store the results for all images\nall_data = []  # Initialise une liste vide pour stocker les résultats de toutes les images\n\n# Iterate through each image in the directory\nfor image_file in tqdm(image_files):  # Parcourt chaque fichier image dans le répertoire, avec une barre de progression\n    # Full path to the image\n    img_path = os.path.join(test_dir_path, image_file)  # Crée le chemin complet vers l'image\n\n    # Make predictions on the image\n    results = model(img_path)  # Effectue des predictions sur l'image à l'aide du modèle\n\n    # Extract bounding boxes, confidence scores, and class labels\n    boxes = results[0].boxes.xyxy.tolist()  # Récupère les boîtes englobantes au format xyxy\n    classes = results[0].boxes.cls.tolist()  # Récupère les indices de classe\n    confidences = results[0].boxes.conf.tolist()  # Récupère les scores de confiance\n    names = results[0].names  # Récupère le dictionnaire des noms de classe\n\n    if not boxes:  # Vérifie s'il n'y a pas de détections\n        # If no detections, add NEG as the class\n        all_data.append({\n            'Image_ID': image_file,  # ID de l'image\n            'class': 'NEG',           # Classe par défaut 'NEG' (négatif)\n            'confidence': 1.0,        # Valeur par défaut (vous pouvez la définir comme valeur standard)\n            'ymin': 0,                # Valeur par défaut (pas de détection)\n            'xmin': 0,                # Valeur par défaut (pas de détection)\n            'ymax': 0,                # Valeur par défaut (pas de détection)\n            'xmax': 0                 # Valeur par défaut (pas de détection)\n        })\n    else:\n        # Iterate through the results for this image\n        for box, cls, conf in zip(boxes, classes, confidences):  # Parcourt les résultats de cette image\n            x1, y1, x2, y2 = box  # Récupère les coordonnées de la boîte englobante\n            detected_class = names[int(cls)]  # Récupère le nom de la classe à partir du dictionnaire de noms\n\n            # Add the result to the all_data list\n            all_data.append({\n                'Image_ID': image_file,  # ID de l'image\n                'class': detected_class,  # Classe détectée\n                'confidence': conf,       # Score de confiance\n                'ymin': y1,               # Coordonnée y minimale\n                'xmin': x1,               # Coordonnée x minimale\n                'ymax': y2,               # Coordonnée y maximale\n                'xmax': x2                # Coordonnée x maximale\n            })\n# Convert the list to a DataFrame for all images\nsub = pd.DataFrame(all_data)","metadata":{"_uuid":"8344f470-7211-40ef-8bfa-dc91dd7c9e3a","_cell_guid":"2f5403c6-439f-46f9-a0cf-32543eab6771","collapsed":false,"id":"CO_SLuUryN0l","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sub.head()  # Affiche les premières lignes du DataFrame 'sub' pour donner un aperçu de son contenu","metadata":{"_uuid":"0ef1d8cf-9672-4603-a16e-5546fd53969e","_cell_guid":"2df4393c-72e3-47f2-9ff0-6beefceccd72","collapsed":false,"id":"HxDKvIQ80W_Z","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sub['class'].value_counts()  # Affiche le nombre d'occurrences de chaque classe dans la colonne 'class' du DataFrame 'sub'","metadata":{"_uuid":"52b00025-ac44-4fb9-bc0b-d2e1d3994710","_cell_guid":"3d64d49d-8967-4327-8f57-c75ebe6cf561","collapsed":false,"id":"F6A2nVrV1-CN","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create submission file to be uploaded to Zindi for scoring\nsub.to_csv('benchmark_submission.csv', index = False)","metadata":{"_uuid":"2f8e10e7-9f55-4316-87de-8161aff89481","_cell_guid":"3987994d-6ba9-49c2-a796-dbbdd48847c8","collapsed":false,"id":"gI-i73Rm2CnH","jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Définir le chemin du fichier d'origine et le chemin de destination\nmodel_path = '/' + os.getcwd() + '/runs/detect/train/weights/best.pt'\ndestination_path = os.getcwd()  # Chemin de destination pour la copie\n\n# Copier le fichier\nshutil.copy(model_path, destination_path)\nprint(f\"Le fichier a été copié dans : {destination_path}\")\n\n# import pandas as pd\n\n# import numpy as np\n\n# # Fonction pour calculer l'AP à 11 points de rappel (VOC 2007)\n# def voc2007_ap(recalls, precisions):\n#     ap = 0.0\n#     for t in np.linspace(0, 1, 11):  # 11 points de rappel\n#         if np.sum(recalls >= t) == 0:\n#             p = 0\n#         else:\n#             p = np.max(precisions[recalls >= t])\n#         ap += p / 11  # moyenne des précisions aux 11 points\n#     return ap\n\n# # Exemple de fonction pour calculer les métriques mAP pour chaque classe\n# def voc2007_mAP(predictions, ground_truths, iou_threshold=0.5):\n#     aps = []\n#     for class_id in np.unique([pred['class_id'] for pred in predictions]):\n#         # Calcul des TP, FP, et du rappel pour la classe en question\n#         recalls, precisions = compute_precision_recall(predictions, ground_truths, class_id, iou_threshold)\n#         ap = voc2007_ap(recalls, precisions)\n#         aps.append(ap)\n#     return np.mean(aps)  # mAP\n\n# def voc2012_ap(recalls, precisions):\n#     # Interpolation pour avoir une courbe de précision décroissante\n#     precisions = np.maximum.accumulate(precisions[::-1])[::-1]\n#     # Calcul de l'aire sous la courbe (AUC) pour obtenir l'AP\n#     ap = np.trapz(precisions, recalls)\n#     return ap\n\n# def voc2012_mAP(predictions, ground_truths, iou_threshold=0.5):\n#     aps = []\n#     for class_id in np.unique([pred['class_id'] for pred in predictions]):\n#         recalls, precisions = compute_precision_recall(predictions, ground_truths, class_id, iou_threshold)\n#         ap = voc2012_ap(recalls, precisions)\n#         aps.append(ap)\n#     return np.mean(aps)\n\n# def coco_ap(recalls, precisions):\n#     precisions = np.maximum.accumulate(precisions[::-1])[::-1]\n#     ap = np.trapz(precisions, recalls)\n#     return ap\n\n# def coco_mAP(predictions, ground_truths):\n#     aps = []\n#     for iou_threshold in np.arange(0.5, 1.0, 0.05):  # Seuils de IoU de 0.50 à 0.95\n#         aps_per_class = []\n#         for class_id in np.unique([pred['class_id'] for pred in predictions]):\n#             recalls, precisions = compute_precision_recall(predictions, ground_truths, class_id, iou_threshold)\n#             ap = coco_ap(recalls, precisions)\n#             aps_per_class.append(ap)\n#         aps.append(np.mean(aps_per_class))\n#     return np.mean(aps)  # mAP\n\n\n# def compute_precision_recall(predictions, ground_truths, class_id, iou_threshold):\n#     # Extraire les prédictions et vérités de terrain pour la classe donnée\n#     preds = [p for p in predictions if p['class_id'] == class_id]\n#     gts = [gt for gt in ground_truths if gt['class_id'] == class_id]\n\n#     # Trier les prédictions par score de confiance décroissant\n#     preds = sorted(preds, key=lambda x: x['confidence'], reverse=True)\n\n#     tp = np.zeros(len(preds))\n#     fp = np.zeros(len(preds))\n#     detected = set()  # Pour suivre les objets déjà détectés\n\n#     # Calcul des TP et FP\n#     for i, pred in enumerate(preds):\n#         best_iou = 0\n#         best_gt = -1\n#         for j, gt in enumerate(gts):\n#             if j in detected:\n#                 continue  # Passer les objets déjà détectés\n\n#             iou = compute_iou(pred['bbox'], gt['bbox'])\n#             if iou > best_iou:\n#                 best_iou = iou\n#                 best_gt = j\n\n#         if best_iou >= iou_threshold:\n#             tp[i] = 1\n#             detected.add(best_gt)\n#         else:\n#             fp[i] = 1\n\n#     # Calcul des rappels et précisions cumulés\n#     tp_cumsum = np.cumsum(tp)\n#     fp_cumsum = np.cumsum(fp)\n#     recalls = tp_cumsum / len(gts)\n#     precisions = tp_cumsum / (tp_cumsum + fp_cumsum)\n\n#     return recalls, precisions\n\n# def compute_iou(boxA, boxB):\n#     xA = max(boxA[0], boxB[0])\n#     yA = max(boxA[1], boxB[1])\n#     xB = min(boxA[2], boxB[2])\n#     yB = min(boxA[3], boxB[3])\n\n#     interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n#     boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n#     boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n\n#     iou = interArea / float(boxAArea + boxBArea - interArea)\n#     return iou\n\n\n# # Charger le fichier de soumission\n# submission_df = pd.read_csv('benchmark_submission.csv')\n# real_df = pd.read_csv('../input/Train.csv')\n\n# # Structure les prédictions\n# predictions = []\n# for _, row in submission_df.iterrows():\n#     bbox = [row['xmin'], row['ymin'], row['xmax'], row['ymax']]\n#     prediction = {\n#         'Image_ID': row['Image_ID'],\n#         'class_id': row['class'],\n#         'confidence': row['confidence'],\n#         'bbox': bbox\n#     }\n#     predictions.append(prediction)\n\n\n# ground_truths = []\n\n# for _, row in real_df.iterrows():\n#     bbox = [row['xmin'], row['ymin'], row['xmax'], row['ymax']]\n#     ground_truth = {\n#         'Image_ID': row['Image_ID'],\n#         'class_id': row['class'],\n#         'confidence': row['confidence'],\n#         'bbox': bbox\n#     }\n#     ground_truths.append(ground_truth)\n\n\n# # Fonction pour calculer mAP selon VOC 2007\n# voc2007_mAP_score = voc2007_mAP(predictions, ground_truths)\n\n# # Fonction pour calculer mAP selon VOC 2012\n# voc2012_mAP_score = voc2012_mAP(predictions, ground_truths)\n\n# # Fonction pour calculer mAP selon COCO\n# coco_mAP_score = coco_mAP(predictions, ground_truths)\n\n# # Affichage des résultats\n# print(\"mAP selon VOC 2007:\", voc2007_mAP_score)\n# print(\"mAP selon VOC 2012:\", voc2012_mAP_score)\n# print(\"mAP selon COCO:\", coco_mAP_score)\n","metadata":{"_uuid":"aa6b88ae-0358-4b4d-b6fe-02dacaed7ddd","_cell_guid":"ac6961b9-97aa-4129-a65b-8d6afda9b0b0","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# train_directory = TRAIN_IMAGES_DIR\n# from skopt import BayesSearchCV\n# from skopt.space import Real, Integer, Categorical\n# from sklearn.base import BaseEstimator\n# from ultralytics import YOLO\n# import os\n# import numpy as np\n\n# class YOLOEstimator(BaseEstimator):\n#     def __init__(self, model_path='yolov9c.pt', data_yaml='data.yaml'):\n#         self.model_path = model_path\n#         self.data_yaml = data_yaml\n#         self.model = None\n\n#     def fit(self, X, y=None, **params):\n#         self.model = YOLO(self.model_path)\n        \n#         # Entraînement avec les hyperparamètres fournis\n#         train_args = {\n#             'data': self.data_yaml,\n#             'epochs': int(params.get('epochs', 20)),\n#             'imgsz': int(params.get('imgsz', 640)),\n#             'batch': int(params.get('batch', 32)),\n#             'device': params.get('device', [0, 1]),\n#             'lr0': float(params.get('lr0', 0.01)),\n#             'lrf': float(params.get('lrf', 0.01)),\n#             'momentum': float(params.get('momentum', 0.937)),\n#             'weight_decay': float(params.get('weight_decay', 0.0005)),\n#             'warmup_epochs': int(params.get('warmup_epochs', 3)),\n#             'warmup_momentum': float(params.get('warmup_momentum', 0.8)),\n#             'warmup_bias_lr': float(params.get('warmup_bias_lr', 0.1)),\n#             'hsv_h': float(params.get('hsv_h', 0.015)),\n#             'hsv_s': float(params.get('hsv_s', 0.7)),\n#             'hsv_v': float(params.get('hsv_v', 0.4)),\n#             'mosaic': float(params.get('mosaic', 1.0)),\n#             'mixup': float(params.get('mixup', 0.5)),\n#             'cache': params.get('cache', True),\n#             'val': params.get('val', True),\n#             'amp': True,\n#             'verbose': False\n#         }\n        \n        \n#         try:\n#             results = self.model.train(**train_args)\n#             self.best_map = results.maps.get('mAP50-95(B)', 0)\n#         except Exception as e:\n#             print(f\"Erreur lors de l'entraînement : {e}\")\n#             self.best_map = 0\n        \n#         return self\n\n#     def score(self, X, y=None):\n#         if self.model is None:\n#             return 0\n#         try:\n#             results = self.model.val()\n#             return results.maps.get('mAP50-95(B)', 0)\n#         except Exception as e:\n#             print(f\"Erreur lors de la validation : {e}\")\n#             return 0\n\n#     def get_params(self, deep=True):\n#         return {\n#             'model_path': self.model_path,\n#             'data_yaml': self.data_yaml\n#         }\n\n#     def set_params(self, **params):\n#         for key, value in params.items():\n#             setattr(self, key, value)\n#         return self\n\n# # Définition de l'espace de recherche pour chaque hyperparamètre à optimiser\n# search_space = {\n#     'epochs': Integer(5, 50),  # Nombre d'époques ajusté\n#     'imgsz': Integer(640, 1024),  # Taille des images d'entrée\n#     'batch': Integer(16, 32),  # Taille du lot ajustée\n#     'lr0': Real(1e-4, 1e-2),          # Taux d'apprentissage initial\n#     'weight_decay': Real(1e-5, 1e-3), # Décroissance des poids\n#     'momentum': Real(0.7, 0.95)       # Momentum\n\n# }\n\n\n\n# # Nombre total d'itérations pour la recherche bayesienne\n# n_iterations = 10\n\n\n# # Récupérer les chemins d'images du dossier d'entraînement\n# X_train = [os.path.join(train_directory, img) for img in os.listdir(train_directory) if img.endswith(('.jpg', '.jpeg', '.png'))]\n# y_train = None  # Pas besoin d'utiliser y_train ici car nous utilisons les chemins directement\n\n# # Entraînement avec YOLOv9 et recherche bayesienne\n# optimizer = BayesSearchCV(\n#     estimator=YOLOEstimator(model_path='yolov9c.pt', data_yaml='data.yaml'),\n#     search_spaces=search_space,\n#     n_iter=n_iterations,  # Nombre d'itérations pour la recherche bayesienne\n#     cv=3,  # Utiliser une validation croisée\n#     n_jobs=-1,  # Utilisation de tous les processeurs disponibles\n#     verbose=False\n# )\n\n# # Ajustement du modèle avec le jeu de données d'entraînement\n# optimizer.fit(X_train, y_train)\n\n# # Meilleurs hyperparamètres trouvés\n# best_hyperparams = optimizer.best_params_\n# print(\"Meilleurs hyperparamètres trouvés : \", best_hyperparams)\n\n#--------------------------------------------------------------------------------------------------\n# import random\n# from deap import base, creator, tools, algorithms\n# from ultralytics import YOLO\n\n# # Fonction pour évaluer le modèle avec les hyperparamètres\n# def evaluate(individual):\n#     # Extraire les valeurs des individus\n#     (optimizer, cos_lr, rect, amp, multi_scale, lr0, lrf, momentum, weight_decay, warmup_epochs,\n#      warmup_momentum, warmup_bias_lr, box, cls, dfl, pose, kobj, label_smoothing,\n#      hsv_h, hsv_s, hsv_v, degrees, translate, scale, shear, perspective, flipud,\n#      fliplr, mosaic, mixup, copy_paste) = individual\n\n#     # Configurer YOLO avec les hyperparamètres\n#     model = YOLO(\"yolov9c.pt\")\n#     train_args = {\n#         'data': 'data.yaml',\n#         'epochs': 100,\n#         'patience': 100,\n#         'batch': 16,\n#         'imgsz': 640,\n#         'save': True,\n#         'optimizer': optimizer,\n#         'cos_lr': cos_lr,\n#         'rect': rect,\n#         'amp': amp,\n#         'multi_scale': multi_scale,\n#         'lr0': lr0,\n#         'lrf': lrf,\n#         'momentum': momentum,\n#         'weight_decay': weight_decay,\n#         'warmup_epochs': warmup_epochs,\n#         'warmup_momentum': warmup_momentum,\n#         'warmup_bias_lr': warmup_bias_lr,\n#         'box': box,\n#         'cls': cls,\n#         'dfl': dfl,\n#         'pose': pose,\n#         'kobj': kobj,\n#         'label_smoothing': label_smoothing,\n#         'hsv_h': hsv_h,\n#         'hsv_s': hsv_s,\n#         'hsv_v': hsv_v,\n#         'degrees': degrees,\n#         'translate': translate,\n#         'scale': scale,\n#         'shear': shear,\n#         'perspective': perspective,\n#         'flipud': flipud,\n#         'fliplr': fliplr,\n#         'mosaic': mosaic,\n#         'mixup': mixup,\n#         'copy_paste': copy_paste\n#     }\n\n#     # Entraîner le modèle\n#     results = model.train(**train_args)\n#     metrics = results.metrics\n\n#     # Retourner la métrique (mAP) pour maximiser\n#     return metrics['map_0.5'],\n\n# # Initialisation de DEAP\n# creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n# creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n\n# toolbox = base.Toolbox()\n# toolbox.register(\"optimizer\", random.choice, [\"SGD\", \"Adam\", \"AdamW\"])\n# toolbox.register(\"cos_lr\", random.choice, [True, False])\n# toolbox.register(\"rect\", random.choice, [True, False])\n# toolbox.register(\"amp\", random.choice, [True, False])\n# toolbox.register(\"multi_scale\", random.choice, [True, False])\n# toolbox.register(\"lr0\", random.uniform, 1e-4, 1e-2)\n# toolbox.register(\"lrf\", random.uniform, 0.01, 0.3)\n# toolbox.register(\"momentum\", random.uniform, 0.85, 0.99)\n# toolbox.register(\"weight_decay\", random.uniform, 1e-5, 1e-3)\n# toolbox.register(\"warmup_epochs\", random.uniform, 0, 5)\n# toolbox.register(\"warmup_momentum\", random.uniform, 0.7, 0.9)\n# toolbox.register(\"warmup_bias_lr\", random.uniform, 0.05, 0.2)\n# toolbox.register(\"box\", random.uniform, 0.02, 0.2)\n# toolbox.register(\"cls\", random.uniform, 0.2, 4.0)\n# toolbox.register(\"dfl\", random.uniform, 0.5, 2.0)\n# toolbox.register(\"pose\", random.uniform, 1.0, 20.0)\n# toolbox.register(\"kobj\", random.uniform, 0.1, 2.0)\n# toolbox.register(\"label_smoothing\", random.uniform, 0.0, 0.1)\n# toolbox.register(\"hsv_h\", random.uniform, 0.0, 0.1)\n# toolbox.register(\"hsv_s\", random.uniform, 0.5, 1.0)\n# toolbox.register(\"hsv_v\", random.uniform, 0.2, 1.0)\n# toolbox.register(\"degrees\", random.uniform, 0, 5)\n# toolbox.register(\"translate\", random.uniform, 0, 0.5)\n# toolbox.register(\"scale\", random.uniform, 0.5, 1.5)\n# toolbox.register(\"shear\", random.uniform, 0.0, 5.0)\n# toolbox.register(\"perspective\", random.uniform, 0.0, 0.001)\n# toolbox.register(\"flipud\", random.uniform, 0.0, 0.5)\n# toolbox.register(\"fliplr\", random.uniform, 0.0, 0.5)\n# toolbox.register(\"mosaic\", random.uniform, 0.0, 1.0)\n# toolbox.register(\"mixup\", random.uniform, 0.0, 1.0)\n# toolbox.register(\"copy_paste\", random.uniform, 0.0, 0.5)\n\n# # Algorithme génétique avec tous les paramètres\n# toolbox.register(\"individual\", tools.initCycle, creator.Individual,\n#                  (toolbox.optimizer, toolbox.cos_lr, toolbox.rect, toolbox.amp, toolbox.multi_scale,\n#                   toolbox.lr0, toolbox.lrf, toolbox.momentum, toolbox.weight_decay, toolbox.warmup_epochs,\n#                   toolbox.warmup_momentum, toolbox.warmup_bias_lr, toolbox.box, toolbox.cls, toolbox.dfl,\n#                   toolbox.pose, toolbox.kobj, toolbox.label_smoothing, toolbox.hsv_h, toolbox.hsv_s,\n#                   toolbox.hsv_v, toolbox.degrees, toolbox.translate, toolbox.scale, toolbox.shear,\n#                   toolbox.perspective, toolbox.flipud, toolbox.fliplr, toolbox.mosaic, toolbox.mixup, toolbox.copy_paste), n=1)\n# toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n# toolbox.register(\"evaluate\", evaluate)\n# toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)\n# toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=0.2, indpb=0.2)\n# toolbox.register(\"select\", tools.selTournament, tournsize=3)\n\n# # Lancer l'algorithme génétique\n# def genetic_algorithm():\n#     population = toolbox.population(n=10)\n#     ngen = 5\n#     cxpb, mutpb = 0.5, 0.2\n#     for gen in range(ngen):\n#         offspring = algorithms.varAnd(population, toolbox, cxpb, mutpb)\n#         fits = map(toolbox.evaluate, offspring)\n#         for fit, ind in zip(fits, offspring):\n#             ind.fitness.values = fit\n#         population = toolbox.select(offspring, k=len(population))\n#     best_ind = tools.selBest(population, 1)[0]\n#     print(\"Best hyperparameters:\", best_ind)\n\n# genetic_algorithm()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}